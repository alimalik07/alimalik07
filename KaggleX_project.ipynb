{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU2zZKHnRMxKeTSGiSM9ZN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimalik07/alimalik07/blob/main/KaggleX_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55CuiqONA9S8",
        "outputId": "5da4db71-4698-4e5e-f02b-e69bc15b9a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip will install package and dependencies for an openai"
      ],
      "metadata": {
        "id": "qNd0SUr4BehT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "kG5dRXQLA-xG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this command will import openai module"
      ],
      "metadata": {
        "id": "q5qaVwEVBnzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "uSllLF5XBC5q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please create your own api key created from openai personal or business account"
      ],
      "metadata": {
        "id": "DWAjRHFiB3Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_generator(topic):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"\"\"You are trained to analyze a topic and generate a tweet.\n",
        "                                        The tweet must contain 300 to 800 words (No less than 400 words).\"\"\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Analyze the topic and generate a tweet. The topic is {topic}\"\"\"}\n",
        "\n",
        "        ]\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "                      model=\"gpt-3.5-turbo\",\n",
        "                      messages=messages,\n",
        "                      max_tokens= 1000,\n",
        "                      n=1,\n",
        "                      stop=None,\n",
        "                      temperature=0.5)\n",
        "\n",
        "    response_text = response.choices[0].message.content.strip().lower()\n",
        "\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "VD8GWsPuBLRJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function will input topic and output a tweet on input topic of length 300 to 800 words"
      ],
      "metadata": {
        "id": "QKqEhP7mCHpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic =  'Features of Kaggle'\n",
        "tweet = tweet_generator(topic)\n",
        "print( 'The generated tweet on topic ', topic, ' is ', tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW2kYZSGBMdI",
        "outputId": "bbc81a3c-1f5e-47af-8ed6-c0d5d10822a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated tweet on topic  Features of Kaggle  is  \"üîçüìä exciting features of kaggle make it the ultimate playground for data scientists and machine learning enthusiasts! üöÄ explore diverse datasets, collaborate with a global community, compete in challenges, and accelerate your ai skills. üåêüèÜ join the data revolution on kaggle today! #datascience #machinelearning #kaggle\"\n"
          ]
        }
      ]
    }
  ]
}